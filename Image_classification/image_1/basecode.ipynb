{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "\n",
    "origin = \"/mnt/c/Users/hajun/Project/image_1/dataset\"\n",
    "base = \"/mnt/c/Users/hajun/Project/image_1/splitted\"\n",
    "clss_list = os.listdir(origin)\n",
    "os.mkdir(base)\n",
    "\n",
    "train_dir = os.path.join(base, \"train\")\n",
    "val_dir = os.path.join(base, \"val\")\n",
    "test_dir = os.path.join(base, \"test\")\n",
    "\n",
    "os.mkdir(train_dir)\n",
    "os.mkdir(val_dir)\n",
    "os.mkdir(test_dir)\n",
    "\n",
    "for clss in clss_list:\n",
    "  os.mkdir(os.path.join(train_dir, clss))\n",
    "  os.mkdir(os.path.join(val_dir, clss))\n",
    "  os.mkdir(os.path.join(test_dir, clss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "for clss in clss_list:\n",
    "  path = os.path.join(origin, clss)\n",
    "  fnames = os.listdir(path)\n",
    "\n",
    "  train_size = math.floor(len(fnames) * 0.6)\n",
    "  val_size = math.floor(len(fnames) * 0.2)\n",
    "  test_size = math.floor(len(fnames) * 0.2)\n",
    "\n",
    "  train_fnames = fnames[:train_size]\n",
    "  for fname in train_fnames:\n",
    "    src = os.path.join(path, fname)\n",
    "    dst = os.path.join(os.path.join(train_dir, clss), fname)\n",
    "    shutil.copyfile(src, dst)\n",
    "    \n",
    "  val_fnames = fnames[train_size : (train_size + val_size)]\n",
    "  for fname in val_fnames:\n",
    "    src = os.path.join(path, fname)\n",
    "    dst = os.path.join(os.path.join(val_dir, clss), fname)\n",
    "    shutil.copyfile(src, dst)\n",
    "  \n",
    "  test_fnames = fnames[(train_size + val_size) : (train_size + val_size + test_size)]\n",
    "  for fname in test_fnames:\n",
    "    src = os.path.join(path, fname)\n",
    "    dst = os.path.join(os.path.join(test_dir, clss), fname)\n",
    "    shutil.copyfile(src, dst)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torchvision.transforms as transforms\n",
    "from torchvision.datasets import ImageFolder\n",
    "\n",
    "use_cuda = torch.cuda.is_available()\n",
    "print(use_cuda)\n",
    "device = torch.device(\"cuda\" if use_cuda == 1 else \"cpu\") # 연산에 사용할 device 정의\n",
    "\n",
    "path = \"/mnt/c/Users/hajun/Project/image_1\"\n",
    "epoch = 10 \n",
    "batch_size = 256\n",
    "\n",
    "transform_base = transforms.Compose([transforms.Resize((64, 64)), transforms.ToTensor()]) \n",
    "# 이미지 전처리, Augmentation등에 활용, 위 코드는 이미지 크기를 (64, 64)로 조정, 이미지 데이터를 tensor로 변환\n",
    "\n",
    "train_dataset = ImageFolder(root = path + \"/splitted/train\", transform = transform_base)\n",
    "val_dataset = ImageFolder(root = path + \"/splitted/val\", transform = transform_base)\n",
    "# 하나의 폴더가 하나의 클래스에 대응하는 구조를 가진 폴더+파일에 사용\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size = batch_size, shuffle = True, num_workers = 4)\n",
    "val_loader = torch.utils.data.DataLoader(val_dataset, batch_size = batch_size, shuffle = True, num_workers = 4)\n",
    "# 불러온 이미지 데이터를 조건에 맞게 미니배치 단위로 분리하는 역할"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 베이스라인 모델 설계\n",
    "\n",
    "import torch.nn as nn    # nn.Module에 딥러닝과 관련된 기본적인 함수들 포함\n",
    "import torch.nn.functional as F  \n",
    "import torch.optim as optim  # 최적화 옵션\n",
    "\n",
    "class Net(nn.Module):  # nn.Module 상속받음 -> 다 끌어와 씀\n",
    "  def __init__(self):\n",
    "    super(Net, self).__init__()\n",
    "    \n",
    "    self.conv1 = nn.Conv2d(3, 32, 3, padding = 1)\n",
    "    self.pool = nn.MaxPool2d(2, 2)\n",
    "    self.conv2 = nn.Conv2d(32, 64, 3, padding = 1)\n",
    "    self.conv3 = nn.Conv2d(64, 64, 3, padding = 1)\n",
    "    # padding=1에 커널 크기3으로 층 지나도 이미지 크기 보존\n",
    "    # Conv2d의 앞 세 개 파라미터 -> 입력 채널 수, 출력 채널 수, 필터사이즈\n",
    "    # 따라서 위 세 개의 합성곱연산을 지나면 채널이 3->32->64->64로, 이미지 크기는 그대로\n",
    "\n",
    "    self.fc1 = nn.Linear(4096, 512) # 합성곱 연산에서 이미지 크기 64*64 --flatten --> 4096을 512로 출력\n",
    "    self.fc2 = nn.Linear(512, 33)  # 512받고  33 -> class 개수로 출력\n",
    "\n",
    "  def forward(self, x):\n",
    "    x = self.conv1(x)\n",
    "    x = F.relu(x)\n",
    "    x = self.pool(x)\n",
    "    x = F.dropout(x, p = 0.25, training = self.training) # 25%의 노드를 dropout, training -> 학습과정에서만 dropout적용\n",
    "\n",
    "    x = self.conv2(x)\n",
    "    x = F.relu(x)\n",
    "    x = self.pool(x)\n",
    "    x = F.dropout(x, p = 0.25, training = self.training) \n",
    "\n",
    "    x = self.conv3(x)\n",
    "    x = F.relu(x)\n",
    "    x = self.pool(x)\n",
    "    x = F.dropout(x, p = 0.25, training = self.training) \n",
    "\n",
    "    x = x.view(-1, 4096) # numpy의 reshape\n",
    "    x = self.fc1(x)\n",
    "    x = F.relu(x)\n",
    "    x = F.dropout(x, p = 0.5, training = self.training) \n",
    "    x = self.fc2(x)\n",
    "\n",
    "    return F.log_softmax(x, dim = 1)\n",
    "\n",
    "model = Net().to(device)\n",
    "optimizer = optim.Adam(model.parameters(), lr = 0.001) # 최적화 옵션으로 Adam사용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 모델 학습을 위한 함수\n",
    "def train(model, train_loader, optimizer):  # 모델 학습에 필요한 것 -> 모델, 데이터, 최적화옵셥\n",
    "  model.train() #모델을 학습모드로 변경\n",
    "  for batch_idx, (data, target) in enumerate(train_loader):  # train_loader는 조건에 맞게 미니배치 단위로 나뉘어져있어 여기 for문처럼 변수할당\n",
    "    data, target = data.to(device), target.to(device)\n",
    "    optimizer.zero_grad() # for문이 배치 단위 -> 이전 배치 Gradient값(가중치 미분값) 초기화\n",
    "    output = model(data)  # 모델에 input넣고 output뽑기\n",
    "    loss = F.cross_entropy(output, target)  # loss값 뽑기\n",
    "    loss.backward()  # 오차역전파\n",
    "    optimizer.step() # 역전파한 값들로 모델 파라미터 업데이트\n",
    "\n",
    "# 모델 평가 함수\n",
    "\n",
    "def evaluate(model, test_loader):\n",
    "  model.eval() # 모델 평가모드로 변경\n",
    "  test_loss = 0\n",
    "  correct = 0\n",
    "\n",
    "  with torch.no_grad():\n",
    "    for data, target in test_loader:\n",
    "      data, target = data.to(device), target.to(device)\n",
    "      output = model(data)\n",
    "\n",
    "      test_loss += F.cross_entropy(output, target, reduction = \"sum\").item()  # loss값은 교차엔트로피 총합으로! -> default는 mean\n",
    "      pred = output.max(1, keepdim = True)[1]\n",
    "      correct += pred.eq(target.view_as(pred)).sum().item()\n",
    "  test_loss /= len(test_loader.dataset) # 이전 test_loss는 배치별 loss총합이 더해진 값. 이를 배치 개수로 나누어 평균 계산\n",
    "  test_accuracy = 100. * correct / len(test_loader.dataset)\n",
    "  return test_loss, test_accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "shape '[-1, 4096]' is invalid for input of size 44282880",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 25\u001b[0m\n\u001b[1;32m     22\u001b[0m   model\u001b[38;5;241m.\u001b[39mload_state_dict(best_model_wts) \u001b[38;5;66;03m# 가장 정확도 높은 모델 반환\u001b[39;00m\n\u001b[1;32m     23\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m model\n\u001b[0;32m---> 25\u001b[0m base \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_baseline\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepoch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     26\u001b[0m torch\u001b[38;5;241m.\u001b[39msave(base, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbaseline.pt\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[0;32mIn[6], line 9\u001b[0m, in \u001b[0;36mtrain_baseline\u001b[0;34m(model, train_loader, val_loader, optimizer, num_epochs)\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m1\u001b[39m, num_epochs\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m):\n\u001b[1;32m      8\u001b[0m   since \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[0;32m----> 9\u001b[0m   \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     10\u001b[0m   train_loss, train_acc \u001b[38;5;241m=\u001b[39m evaluate(model, train_loader)\n\u001b[1;32m     11\u001b[0m   val_loss, val_acc \u001b[38;5;241m=\u001b[39m evaluate(model, val_loader)\n",
      "Cell \u001b[0;32mIn[5], line 7\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(model, train_loader, optimizer)\u001b[0m\n\u001b[1;32m      5\u001b[0m data, target \u001b[38;5;241m=\u001b[39m data\u001b[38;5;241m.\u001b[39mto(device), target\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m      6\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad() \u001b[38;5;66;03m# for문이 배치 단위 -> 이전 배치 Gradient값(가중치 미분값) 초기화\u001b[39;00m\n\u001b[0;32m----> 7\u001b[0m output \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# 모델에 input넣고 output뽑기\u001b[39;00m\n\u001b[1;32m      8\u001b[0m loss \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39mcross_entropy(output, target)  \u001b[38;5;66;03m# loss값 뽑기\u001b[39;00m\n\u001b[1;32m      9\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()  \u001b[38;5;66;03m# 오차역전파\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/image/lib/python3.12/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/image/lib/python3.12/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[4], line 38\u001b[0m, in \u001b[0;36mNet.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     35\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpool(x)\n\u001b[1;32m     36\u001b[0m x \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39mdropout(x, p \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.25\u001b[39m, training \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtraining) \n\u001b[0;32m---> 38\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[43mx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mview\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m4096\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;66;03m# numpy의 reshape\u001b[39;00m\n\u001b[1;32m     39\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfc1(x)\n\u001b[1;32m     40\u001b[0m x \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39mrelu(x)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: shape '[-1, 4096]' is invalid for input of size 44282880"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import copy\n",
    "def train_baseline(model, train_loader, val_loader, optimizer, num_epochs = 30):\n",
    "  best_acc = 0.0\n",
    "  best_model_wts = copy.deepcopy(model.state_dict()) # 정확도 가장 높은 모델 저장\n",
    "\n",
    "  for epoch in range(1, num_epochs+1):\n",
    "    since = time.time()\n",
    "    train(model, train_loader, optimizer)\n",
    "    train_loss, train_acc = evaluate(model, train_loader)\n",
    "    val_loss, val_acc = evaluate(model, val_loader)\n",
    "\n",
    "    if val_acc > best_acc:\n",
    "      best_acc = val_acc\n",
    "      best_model_wts = copy.deepcopy(model.state_dict())\n",
    "\n",
    "    time_elapsed = time.time() - since\n",
    "    print(f\"------------------------------------- epoch {epoch} -------------------------------------\")\n",
    "    print(\"train Loss : {:.4f}, Accuracy : {:.2f}%\".format(train_loss, train_acc))\n",
    "    print(\"val_Loss : {:.4f}, Accuracy : {:.2f}%\".format(val_loss, val_acc))\n",
    "    print(\"Completed in {:.0f}m {:.0f}s\".format(time_elapsed//60, time_elapsed % 60))\n",
    "  model.load_state_dict(best_model_wts) # 가장 정확도 높은 모델 반환\n",
    "  return model\n",
    "\n",
    "base = train_baseline(model, train_loader, val_loader, optimizer, epoch)\n",
    "torch.save(base, \"baseline.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision.transforms as transforms\n",
    "from torchvision.datasets import ImageFolder\n",
    "import os\n",
    "\n",
    "use_cuda = torch.cuda.is_available()\n",
    "device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n",
    "\n",
    "batch_size = 256\n",
    "epoch = 30\n",
    "\n",
    "data_transforms = {\n",
    "    \"train\" : transforms.Compose([\n",
    "        transforms.Resize([64, 64]),  \n",
    "        transforms.RandomHorizontalFlip(),  # base model과 다르게 augmentation을 적용하는데, \n",
    "        transforms.RandomVerticalFlip(),    # pre_trained model이라 데이터 양을 더 늘린 것 같음(자료에 이유가 안 나와있다..) \n",
    "        transforms.RandomCrop(52),  # 이미지 일부를 52*52로 잘라내어씀\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.485, 0.456, 0.406],  # 정규화에 쓰일 각 채널의 평균값\n",
    "                            [0.229, 0.224, 0.225])   # 정규화에 쓰일 각 채널의 표준편차값\n",
    "    ]),\n",
    "    \"val\" : transforms.Compose([\n",
    "        transforms.Resize([64, 64]),\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        transforms.RandomVerticalFlip(),\n",
    "        transforms.RandomCrop(52),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.485, 0.456, 0.406],\n",
    "                            [0.229, 0.224, 0.225])\n",
    "    ]),\n",
    "    \n",
    "}\n",
    "\n",
    "data_dir = path + \"/splitted\"\n",
    "\n",
    "dataset = {x : ImageFolder(root = os.path.join(data_dir, x), transform = data_transforms[x]) for x in [\"train\", \"val\"]}\n",
    "loader = {x : torch.utils.data.DataLoader(dataset[x], batch_size = batch_size, shuffle = True, num_workers = 4) for x in [\"train\", \"val\"]}\n",
    "dataset_sizes = {x : len(dataset[x]) for x in [\"train\", \"val\"]}\n",
    "class_names = dataset[\"train\"].classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hajun/anaconda3/envs/image/lib/python3.12/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/home/hajun/anaconda3/envs/image/lib/python3.12/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet50_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet50_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n",
      "Downloading: \"https://download.pytorch.org/models/resnet50-0676ba61.pth\" to /home/hajun/.cache/torch/hub/checkpoints/resnet50-0676ba61.pth\n",
      "100.0%\n"
     ]
    }
   ],
   "source": [
    "from torchvision import models\n",
    "resnet = models.resnet50(pretrained = True) # False가 되면 모델의 구조만 가져오고 초깃값은 랜덤 설정\n",
    "num_ftrs = resnet.fc.in_features # fc는 모델의 마지막 layer를, in_features는 해당 층의 입력 채널 수 반환\n",
    "resnet.fc = nn.Linear(num_ftrs, 33) # 마지막 fc층의 출력 채널을 클래스 수에 맞게 변환\n",
    "resnet = resnet.to(device)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer_ft = optim.Adam(filter(lambda p : p.requires_grad, resnet.parameters()), lr=0.001)\n",
    "# filter와 lambda를 통해 requires_grad=True인 layer만 파라미터 업데이트\n",
    "\n",
    "from torch.optim import lr_scheduler\n",
    "exp_lr_scheduler = lr_scheduler.StepLR(optimizer_ft, step_size = 7, gamma = 0.1)\n",
    "# 에폭에 따라 lr변경 -> 7에폭마다 0.1씩 곱해짐"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_resnet(model, criterion, optimizer, scheduler, num_epochs = 25):\n",
    "    \n",
    "    best_model_wts = copy.deepcopy(model.state_dict())\n",
    "    best_acc = 0\n",
    "    \n",
    "    for epoch in range(num_epochs):                           # 에폭마다 for문\n",
    "        print(f\"---------- epoch {epoch + 1} ----------\")\n",
    "        since = time.time()\n",
    "        \n",
    "        for phase in [\"train\", \"val\"]:                        # train과 val데이터 동시에-> for문인거 신경 안써도됨\n",
    "            if phase == \"train\":\n",
    "                model.train()\n",
    "            else:\n",
    "                model.eval()\n",
    "            running_loss = 0.0\n",
    "            running_corrects = 0.0\n",
    "            \n",
    "            for inputs, labels in loader[phase]:             # 배치단위마다 for문\n",
    "                inputs = inputs.to(device)\n",
    "                labels = labels.to(device)\n",
    "                \n",
    "                optimizer.zero_grad()\n",
    "                \n",
    "                with torch.set_grad_enabled(phase == \"train\"):\n",
    "                    outputs = model(inputs)\n",
    "                    x, preds = torch.max(outputs, 1)\n",
    "                    loss = criterion(outputs, labels)\n",
    "                    \n",
    "                    if phase == \"train\":\n",
    "                        loss.backward() \n",
    "                        optimizer.step()       \n",
    "                        \n",
    "                running_loss += loss.item() * inputs.size(0)       # 교차엔트로피 계산 deafualt값이 mean이므로 각 데이터 마다의 손실 평균이 저장되있음\n",
    "                                                                   # 따라서 배치 사이즈를 곱해줘 한 배치 사이즈의 loss 총합을 계산!\n",
    "                running_corrects += torch.sum(preds == labels.data)  # -----------------여기까진 base model과 구조가 동일----------------------------\n",
    "            if phase == \"train\":\n",
    "                scheduler.step()\n",
    "                l_r = [x[\"lr\"] for x in optimizer_ft.param_groups]\n",
    "                print(\"learning rate : \", l_r)\n",
    "                 \n",
    "            epoch_loss = running_loss/dataset_sizes[phase]          # 전체 데이터 loss합을 각 데이터셋 전체 크기로 나눠주어 loss계산\n",
    "            epoch_acc = running_corrects.double() / dataset_sizes[phase]\n",
    "            \n",
    "            print(\"{} Loss: {:4f} Acc: {:.4f}\".format(phase, epoch_loss, epoch_acc))\n",
    "            \n",
    "            if phase == \"val\" and epoch_acc > best_acc:\n",
    "                best_acc = epoch_acc\n",
    "                best_model_wts = copy.deepcopy(model.state_dict())\n",
    "                \n",
    "        time_elapsed = time.time() - since\n",
    "        print(\"Completed in {:.0f}m {:0f}s\".format(time_elapsed // 60, time_elapsed % 60))\n",
    "        \n",
    "    print(\"Best val Acc: {:.4f}\".format(best_acc))\n",
    "    \n",
    "    model.load_state_dict(best_model_wts)\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------- epoch 1 ----------\n",
      "learning rate :  [0.001]\n",
      "train Loss: 3.354830 Acc: 0.0611\n",
      "val Loss: 2.470476 Acc: 0.4500\n",
      "Completed in 0m 4.852816s\n",
      "---------- epoch 2 ----------\n",
      "learning rate :  [0.001]\n",
      "train Loss: 1.296646 Acc: 0.7278\n",
      "val Loss: 3.960766 Acc: 0.4500\n",
      "Completed in 0m 4.685889s\n",
      "---------- epoch 3 ----------\n",
      "learning rate :  [0.001]\n",
      "train Loss: 0.521374 Acc: 0.8333\n",
      "val Loss: 2.866120 Acc: 0.5500\n",
      "Completed in 0m 4.757969s\n",
      "---------- epoch 4 ----------\n",
      "learning rate :  [0.001]\n",
      "train Loss: 0.430191 Acc: 0.8611\n",
      "val Loss: 0.792002 Acc: 0.8167\n",
      "Completed in 0m 4.926021s\n",
      "---------- epoch 5 ----------\n",
      "learning rate :  [0.001]\n",
      "train Loss: 0.337188 Acc: 0.8611\n",
      "val Loss: 1.604713 Acc: 0.8000\n",
      "Completed in 0m 4.419656s\n",
      "---------- epoch 6 ----------\n",
      "learning rate :  [0.001]\n",
      "train Loss: 0.267165 Acc: 0.9111\n",
      "val Loss: 0.606081 Acc: 0.8833\n",
      "Completed in 0m 5.123018s\n",
      "---------- epoch 7 ----------\n",
      "learning rate :  [0.0001]\n",
      "train Loss: 0.139732 Acc: 0.9333\n",
      "val Loss: 1.832132 Acc: 0.7833\n",
      "Completed in 0m 3.655727s\n",
      "---------- epoch 8 ----------\n",
      "learning rate :  [0.0001]\n",
      "train Loss: 0.156213 Acc: 0.9500\n",
      "val Loss: 0.991796 Acc: 0.8333\n",
      "Completed in 0m 4.769250s\n",
      "---------- epoch 9 ----------\n",
      "learning rate :  [0.0001]\n",
      "train Loss: 0.122516 Acc: 0.9556\n",
      "val Loss: 0.937564 Acc: 0.8833\n",
      "Completed in 0m 3.627068s\n",
      "---------- epoch 10 ----------\n",
      "learning rate :  [0.0001]\n",
      "train Loss: 0.125216 Acc: 0.9500\n",
      "val Loss: 0.520080 Acc: 0.9000\n",
      "Completed in 0m 4.492760s\n",
      "---------- epoch 11 ----------\n",
      "learning rate :  [0.0001]\n",
      "train Loss: 0.089090 Acc: 0.9556\n",
      "val Loss: 0.647861 Acc: 0.8667\n",
      "Completed in 0m 3.626067s\n",
      "---------- epoch 12 ----------\n",
      "learning rate :  [0.0001]\n",
      "train Loss: 0.069307 Acc: 0.9833\n",
      "val Loss: 0.696585 Acc: 0.8500\n",
      "Completed in 0m 4.145507s\n",
      "---------- epoch 13 ----------\n",
      "learning rate :  [0.0001]\n",
      "train Loss: 0.072567 Acc: 0.9778\n",
      "val Loss: 0.290870 Acc: 0.9167\n",
      "Completed in 0m 3.382027s\n",
      "---------- epoch 14 ----------\n",
      "learning rate :  [1e-05]\n",
      "train Loss: 0.043734 Acc: 0.9889\n",
      "val Loss: 0.421329 Acc: 0.8833\n",
      "Completed in 0m 3.904184s\n",
      "---------- epoch 15 ----------\n",
      "learning rate :  [1e-05]\n",
      "train Loss: 0.062369 Acc: 0.9722\n",
      "val Loss: 0.140106 Acc: 0.9333\n",
      "Completed in 0m 4.710473s\n",
      "---------- epoch 16 ----------\n",
      "learning rate :  [1e-05]\n",
      "train Loss: 0.058289 Acc: 0.9778\n",
      "val Loss: 0.154844 Acc: 0.9500\n",
      "Completed in 0m 4.834628s\n",
      "---------- epoch 17 ----------\n",
      "learning rate :  [1e-05]\n",
      "train Loss: 0.057929 Acc: 0.9722\n",
      "val Loss: 0.316843 Acc: 0.9000\n",
      "Completed in 0m 3.984686s\n",
      "---------- epoch 18 ----------\n",
      "learning rate :  [1e-05]\n",
      "train Loss: 0.055091 Acc: 0.9833\n",
      "val Loss: 0.209380 Acc: 0.9167\n",
      "Completed in 0m 4.768066s\n",
      "---------- epoch 19 ----------\n",
      "learning rate :  [1e-05]\n",
      "train Loss: 0.034075 Acc: 0.9944\n",
      "val Loss: 0.153221 Acc: 0.9167\n",
      "Completed in 0m 5.130563s\n",
      "---------- epoch 20 ----------\n",
      "learning rate :  [1e-05]\n",
      "train Loss: 0.049530 Acc: 0.9833\n",
      "val Loss: 0.382248 Acc: 0.8667\n",
      "Completed in 0m 4.532331s\n",
      "---------- epoch 21 ----------\n",
      "learning rate :  [1.0000000000000002e-06]\n",
      "train Loss: 0.053320 Acc: 0.9778\n",
      "val Loss: 0.266736 Acc: 0.8833\n",
      "Completed in 0m 4.597941s\n",
      "---------- epoch 22 ----------\n",
      "learning rate :  [1.0000000000000002e-06]\n",
      "train Loss: 0.059251 Acc: 0.9778\n",
      "val Loss: 0.181335 Acc: 0.9333\n",
      "Completed in 0m 5.126850s\n",
      "---------- epoch 23 ----------\n",
      "learning rate :  [1.0000000000000002e-06]\n",
      "train Loss: 0.051472 Acc: 0.9778\n",
      "val Loss: 0.221112 Acc: 0.9167\n",
      "Completed in 0m 4.253578s\n",
      "---------- epoch 24 ----------\n",
      "learning rate :  [1.0000000000000002e-06]\n",
      "train Loss: 0.030665 Acc: 0.9944\n",
      "val Loss: 0.135701 Acc: 0.9500\n",
      "Completed in 0m 4.672143s\n",
      "---------- epoch 25 ----------\n",
      "learning rate :  [1.0000000000000002e-06]\n",
      "train Loss: 0.039409 Acc: 0.9778\n",
      "val Loss: 0.160327 Acc: 0.9000\n",
      "Completed in 0m 4.668925s\n",
      "---------- epoch 26 ----------\n",
      "learning rate :  [1.0000000000000002e-06]\n",
      "train Loss: 0.046470 Acc: 0.9889\n",
      "val Loss: 0.161938 Acc: 0.9500\n",
      "Completed in 0m 4.671175s\n",
      "---------- epoch 27 ----------\n",
      "learning rate :  [1.0000000000000002e-06]\n",
      "train Loss: 0.057626 Acc: 0.9889\n",
      "val Loss: 0.109380 Acc: 0.9500\n",
      "Completed in 0m 4.195291s\n",
      "---------- epoch 28 ----------\n",
      "learning rate :  [1.0000000000000002e-07]\n",
      "train Loss: 0.033453 Acc: 0.9889\n",
      "val Loss: 0.179160 Acc: 0.9333\n",
      "Completed in 0m 5.014053s\n",
      "---------- epoch 29 ----------\n",
      "learning rate :  [1.0000000000000002e-07]\n",
      "train Loss: 0.063960 Acc: 0.9722\n",
      "val Loss: 0.188272 Acc: 0.9000\n",
      "Completed in 0m 4.928960s\n",
      "---------- epoch 30 ----------\n",
      "learning rate :  [1.0000000000000002e-07]\n",
      "train Loss: 0.054583 Acc: 0.9889\n",
      "val Loss: 0.215789 Acc: 0.9000\n",
      "Completed in 0m 3.757982s\n",
      "Best val Acc: 0.9500\n"
     ]
    }
   ],
   "source": [
    "model_resnet50 = train_resnet(resnet, criterion, optimizer_ft, exp_lr_scheduler, num_epochs=epoch)\n",
    "torch.save(model_resnet50, \"resnet50.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "image",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
